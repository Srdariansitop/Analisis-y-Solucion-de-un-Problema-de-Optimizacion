{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8927fe51",
   "metadata": {},
   "source": [
    "Aplicacion del Algoritmo BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Definición de la función, gradiente y log-sum-exp seguro\n",
    "# ---------------------------------------------------------\n",
    "def f(x):\n",
    "    # x es un vector [x0, x1]\n",
    "    x1, x2 = x\n",
    "    A = np.exp(x1**2 + x2**2) + 10 * np.exp(x1)\n",
    "    return np.log(A)\n",
    "\n",
    "def grad_f(x):\n",
    "    x1, x2 = x\n",
    "    A = np.exp(x1**2 + x2**2) + 10 * np.exp(x1)\n",
    "    dfdx1 = (2 * x1 * np.exp(x1**2 + x2**2) + 10 * np.exp(x1)) / A\n",
    "    dfdx2 = (2 * x2 * np.exp(x1**2 + x2**2)) / A\n",
    "    return np.array([dfdx1, dfdx2])\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Búsqueda de línea con condición de Armijo\n",
    "# ---------------------------------------------------------\n",
    "def line_search_armijo(f, grad_f, xk, pk, alpha0=1.0, c=1e-4, rho=0.5):\n",
    "    alpha = alpha0\n",
    "    fk = f(xk)\n",
    "    gradk = grad_f(xk)\n",
    "    while f(xk + alpha * pk) > fk + c * alpha * np.dot(gradk, pk):\n",
    "        alpha *= rho\n",
    "        if alpha < 1e-10:\n",
    "            break\n",
    "    return alpha\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Método BFGS\n",
    "# ---------------------------------------------------------\n",
    "def bfgs(f, grad_f, x0, tol=1e-6, max_iter=1000):\n",
    "    xk = x0.copy()\n",
    "    n = len(xk)\n",
    "    Hk = np.eye(n)  # matriz de aproximación del inverso del Hessiano\n",
    "    fk = f(xk)\n",
    "    gk = grad_f(xk)\n",
    "    iter_data = [(0, xk.copy(), fk, np.linalg.norm(gk))]\n",
    "\n",
    "    for k in range(1, max_iter + 1):\n",
    "        # Dirección de búsqueda\n",
    "        pk = -Hk.dot(gk)\n",
    "\n",
    "        # Búsqueda de línea (Armijo)\n",
    "        alpha = line_search_armijo(f, grad_f, xk, pk)\n",
    "\n",
    "        # Actualización de x\n",
    "        x_new = xk + alpha * pk\n",
    "        g_new = grad_f(x_new)\n",
    "        s = x_new - xk\n",
    "        y = g_new - gk\n",
    "\n",
    "        # Condición de actualización BFGS (evitar divisiones malas)\n",
    "        if np.dot(y, s) > 1e-10:\n",
    "            rho = 1.0 / np.dot(y, s)\n",
    "            I = np.eye(n)\n",
    "            Hk = (I - rho * np.outer(s, y)) @ Hk @ (I - rho * np.outer(y, s)) + rho * np.outer(s, s)\n",
    "\n",
    "        # Actualizar variables\n",
    "        xk, gk, fk = x_new, g_new, f(x_new)\n",
    "        iter_data.append((k, xk.copy(), fk, np.linalg.norm(gk)))\n",
    "\n",
    "        # Criterios de paro\n",
    "        if np.linalg.norm(gk) < tol:\n",
    "            break\n",
    "\n",
    "    return xk, fk, np.linalg.norm(gk), k, iter_data\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Ejemplo de ejecución\n",
    "# ---------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    x0 = np.array([0.0, 0.0])   # punto inicial\n",
    "    x_opt, f_opt, grad_norm, iters, history = bfgs(f, grad_f, x0)\n",
    "\n",
    "    print(\"Resultado BFGS:\")\n",
    "    print(f\"  x* = {x_opt}\")\n",
    "    print(f\"  f(x*) = {f_opt:.6f}\")\n",
    "    print(f\"  ||grad|| = {grad_norm:.2e}\")\n",
    "    print(f\"  Iteraciones = {iters}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5fbdb8",
   "metadata": {},
   "source": [
    "El script está organizado en tres bloques funcionales:\n",
    "\n",
    "Definición del problema: f(x) y grad_f(x).\n",
    "\n",
    "Búsqueda de línea: line_search_armijo(...).\n",
    "\n",
    "Algoritmo BFGS: bfgs(...) que llama a la búsqueda de línea y actualiza la aproximación del inverso del Hessiano.\n",
    "Al final hay un if __name__ == \"__main__\": con un ejemplo de ejecución."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c8ab31",
   "metadata": {},
   "source": [
    "1. Explicacion de la funcion :\n",
    "\n",
    "Qué hace: grad_f(x) Calcula ∇f usando las fórmulas analíticas que derivamos.\n",
    "\n",
    "Por qué usar el gradiente analítico: BFGS es un método cuasi-Newton que requiere gradientes para construir las actualizaciones. Calcular el gradiente analíticamente es más preciso y rápido que aproximarlo por diferencias finitas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5691d9",
   "metadata": {},
   "source": [
    "2. Explicacion de la funcion :\n",
    "\n",
    "line_search_armijo(...) — búsqueda de línea simple (Armijo / backtracking)\n",
    "\n",
    "Qué hace: \n",
    "- Intenta alpha=1 y si el nuevo punto no satisface la condición de Armijo (descenso suficiente), reduce alpha multiplicándolo por rho (0.5) repetidamente (backtracking).\n",
    "\n",
    "Parámetros importantes:\n",
    "\n",
    "c: constante de Armijo (típico 10^(-4))\n",
    "\n",
    "rho: factor de reducción (0.5) — cada iteración divide alpha por 2.\n",
    "\n",
    "Por qué usar Armijo simple: es fácil de implementar y suele ser suficiente para BFGS en problemas suaves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429d9f79",
   "metadata": {},
   "source": [
    "3. Explicacion de la funcion :\n",
    "\n",
    "bfgs(f, grad_f, x0, tol=1e-6, max_iter=1000): - El nucleo del algoritmo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
