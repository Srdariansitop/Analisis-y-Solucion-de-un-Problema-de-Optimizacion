{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8927fe51",
   "metadata": {},
   "source": [
    "Aplicacion del Algoritmo BFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f12e87e",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# =========================================================\n",
    "# Función y gradiente con estabilización numérica (log-sum-exp)\n",
    "# =========================================================\n",
    "def f(x):\n",
    "    x1, x2 = x\n",
    "    # Evitar overflow: usar log-sum-exp\n",
    "    a = x1**2 + x2**2\n",
    "    b = x1 + np.log(10)\n",
    "    M = max(a, b)\n",
    "    return M + np.log(np.exp(a - M) + np.exp(b - M))\n",
    "\n",
    "def grad_f(x):\n",
    "    x1, x2 = x\n",
    "    a = x1**2 + x2**2\n",
    "    b = x1 + np.log(10)\n",
    "    M = max(a, b)\n",
    "    # pesos numéricamente estables\n",
    "    wa = np.exp(a - M)\n",
    "    wb = np.exp(b - M)\n",
    "    denom = wa + wb\n",
    "    # gradiente derivado de la forma log-sum-exp\n",
    "    dfdx1 = (2 * x1 * wa + wb) / denom\n",
    "    dfdx2 = (2 * x2 * wa) / denom\n",
    "    return np.array([dfdx1, dfdx2])\n",
    "\n",
    "# =========================================================\n",
    "# Búsqueda de línea de Armijo\n",
    "# =========================================================\n",
    "def line_search_armijo(f, grad_f, xk, pk, alpha0=1.0, c=1e-4, rho=0.5):\n",
    "    alpha = alpha0\n",
    "    fk = f(xk)\n",
    "    gradk = grad_f(xk)\n",
    "    while f(xk + alpha * pk) > fk + c * alpha * np.dot(gradk, pk):\n",
    "        alpha *= rho\n",
    "        if alpha < 1e-10:\n",
    "            break\n",
    "    return alpha\n",
    "\n",
    "# =========================================================\n",
    "# Método BFGS\n",
    "# =========================================================\n",
    "def bfgs(f, grad_f, x0, tol=1e-6, max_iter=1000):\n",
    "    xk = x0.copy()\n",
    "    n = len(xk)\n",
    "    Hk = np.eye(n)\n",
    "    fk = f(xk)\n",
    "    gk = grad_f(xk)\n",
    "    iter_data = []\n",
    "\n",
    "    for k in range(1, max_iter + 1):\n",
    "        pk = -Hk.dot(gk)\n",
    "        alpha = line_search_armijo(f, grad_f, xk, pk)\n",
    "        x_new = xk + alpha * pk\n",
    "        g_new = grad_f(x_new)\n",
    "        s = x_new - xk\n",
    "        y = g_new - gk\n",
    "\n",
    "        if np.dot(y, s) > 1e-10:\n",
    "            rho = 1.0 / np.dot(y, s)\n",
    "            I = np.eye(n)\n",
    "            Hk = (I - rho * np.outer(s, y)) @ Hk @ (I - rho * np.outer(y, s)) + rho * np.outer(s, s)\n",
    "\n",
    "        xk, gk, fk = x_new, g_new, f(x_new)\n",
    "        iter_data.append({\n",
    "            \"iter\": k,\n",
    "            \"x\": xk.tolist(),\n",
    "            \"f\": float(fk),\n",
    "            \"grad_norm\": float(np.linalg.norm(gk)),\n",
    "            \"alpha\": float(alpha)\n",
    "        })\n",
    "\n",
    "        if np.linalg.norm(gk) < tol:\n",
    "            break\n",
    "\n",
    "    return xk, fk, np.linalg.norm(gk), k, iter_data\n",
    "\n",
    "# =========================================================\n",
    "# Probar en 100 puntos aleatorios\n",
    "# =========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    import json\n",
    "\n",
    "    # Cargar puntos iniciales previamente guardados\n",
    "    with open(\"initial_points.json\", \"r\") as f_in:\n",
    "        points = np.array(json.load(f_in))\n",
    "\n",
    "    results = []\n",
    "    for i, x0 in enumerate(points):\n",
    "        try:\n",
    "            x_opt, f_opt, grad_norm, iters, history = bfgs(f, grad_f, x0)\n",
    "            results.append({\n",
    "                \"id\": i + 1,\n",
    "                \"x0\": x0.tolist(),\n",
    "                \"x_opt\": x_opt.tolist(),\n",
    "                \"f_opt\": float(f_opt),\n",
    "                \"grad_norm\": float(grad_norm),\n",
    "                \"iters\": int(iters),\n",
    "                \"success\": True\n",
    "            })\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                \"id\": i + 1,\n",
    "                \"x0\": x0.tolist(),\n",
    "                \"error\": str(e),\n",
    "                \"success\": False\n",
    "            })\n",
    "\n",
    "    with open(\"bfgs_results.json\", \"w\") as f_out:\n",
    "        json.dump(results, f_out, indent=4)\n",
    "\n",
    "    print(\"✅ Resultados BFGS guardados en 'bfgs_results.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5fbdb8",
   "metadata": {},
   "source": [
    "El script está organizado en tres bloques funcionales:\n",
    "\n",
    "- Definición del problema: f(x) y grad_f(x).\n",
    "\n",
    "- Búsqueda de línea: line_search_armijo(...).\n",
    "\n",
    "- Algoritmo BFGS: bfgs(...) que llama a la búsqueda de línea y actualiza la aproximación del inverso del Hessiano.\n",
    "\n",
    "- Al final hay un if __name__ == \"__main__\": con un ejemplo de ejecución."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c8ab31",
   "metadata": {},
   "source": [
    "1. Explicacion de la funcion :\n",
    "\n",
    "Qué hace: grad_f(x) \n",
    "- Calcula ∇f usando las fórmulas analíticas que derivamos.\n",
    "\n",
    "Por qué usar el gradiente analítico: \n",
    "\n",
    "- BFGS es un método cuasi-Newton que requiere gradientes para construir las actualizaciones. \n",
    "- Calcular el gradiente analíticamente es más preciso y rápido que aproximarlo por diferencias finitas.\n",
    "\n",
    "- La implementación usa la técnica Log-Sum-Exp (LSE) para evitar desbordamiento numérico (overflow) cuando los argumentos del exponencial son grandes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5691d9",
   "metadata": {},
   "source": [
    "2. Explicacion de la funcion :\n",
    "\n",
    "line_search_armijo(...) — búsqueda de línea simple (Armijo / backtracking)\n",
    "\n",
    "Qué hace: \n",
    "- Intenta alpha=1 y si el nuevo punto no satisface la condición de Armijo (descenso suficiente), reduce alpha multiplicándolo por rho (0.5) repetidamente (backtracking).\n",
    "\n",
    "Parámetros importantes:\n",
    "\n",
    "- c: constante de Armijo (típico 10^(-4))\n",
    "\n",
    "- rho: factor de reducción (0.5) — cada iteración divide alpha por 2.\n",
    "\n",
    "Por qué usar Armijo simple: \n",
    "- es fácil de implementar y suele ser suficiente para BFGS en problemas suaves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429d9f79",
   "metadata": {},
   "source": [
    "3. Explicacion de la funcion :\n",
    "\n",
    "bfgs(f, grad_f, x0, tol=1e-6, max_iter=1000): - El nucleo del algoritmo\n",
    "\n",
    "Inicialización\n",
    "\n",
    "- Hk = I: asumimos inicialmente que el inverso del Hessiano es identidad (solución neutra).\n",
    "\n",
    "- xk : Copia del punto inicial\n",
    "\n",
    "- gk = grad_f(xk): empezamos con el gradiente en el punto inicial.\n",
    "\n",
    "- fk : Valor de la función en xk\n",
    "\n",
    "- iter_data: se guarda historia para análisis o trazados.\n",
    "\n",
    "Dirección de búsqueda pk\n",
    "\n",
    "- pk = -Hk.dot(gk) produce una dirección que incorpora información aproximada de curvatura; si Hk=I, pk es la dirección de descenso por gradiente.\n",
    "\n",
    "Búsqueda de línea\n",
    "\n",
    "- Se usa la función Armijo descrita. El tamaño alpha determina cuánto avanzar.\n",
    "\n",
    "Actualizar Actualizar el Punto\n",
    "\n",
    "- x_new: Nuevo punto en el espacio de búsqueda\n",
    "\n",
    "- g_new: Gradiente en el nuevo punto (necesario para BFGS)\n",
    "\n",
    "Calcular Diferencias Clave .Interpretación geométrica:\n",
    "\n",
    "- s: Vector que conecta el punto anterior con el nuevo\n",
    "\n",
    "- y: Diferencia entre gradientes, relacionada con la curvatura\n",
    "\n",
    "- La relación y ≈ H·s es la ecuación secante que BFGS quiere satisfacer\n",
    "\n",
    "Condición de Curvatura y Actualización BFGS , Análisis de la condición np.dot(y, s) > 1e-10:\n",
    "\n",
    "- Garantiza que la actualización sea numéricamente estable\n",
    "\n",
    "- y·s > 0 asegura condición de curvatura positiva\n",
    "\n",
    "- Si y·s ≤ 0, la función no es \"convexa localmente\" en esa dirección\n",
    "\n",
    "- En ese caso, se salta la actualización de Hk\n",
    "\n",
    "Fórmula matemática:\n",
    "\n",
    "H_{k+1} = (I - ρ·s·yᵀ) · H_k · (I - ρ·y·sᵀ) + ρ·s·sᵀ\n",
    "\n",
    "¿Por qué esta fórmula?:\n",
    "\n",
    "- Mantiene Hk simétrica y definida positiva (si inicialmente lo es)\n",
    "\n",
    "- Satisface la ecuación secante: H_{k+1}·y = s\n",
    "\n",
    "- Aprende la curvatura local de la función a partir de gradientes\n",
    "\n",
    "Criterio de parada: Norma del gradiente menor que tolerancia\n",
    "\n",
    "- Si ‖∇f(x)‖ < tol, estamos cerca de un punto estacionario (mínimo local)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000e9851",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de366071",
   "metadata": {},
   "source": [
    "Algoritmo de Newton amortiguado (Damped Newton / Trust-Region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b942ed10",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9825f19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluación numéricamente estable de f, gradiente y Hessiano\n",
    "# f(x,y) = ln(e^{x^2+y^2} + 10 e^{x})\n",
    "# Usamos log-sum-exp para evitar overflow.\n",
    "# -----------------------------\n",
    "def f_stable(x):\n",
    "    x1, x2 = float(x[0]), float(x[1])\n",
    "    a = x1**2 + x2**2\n",
    "    b = x1 + np.log(10.0)    # log(10 * e^{x1}) = x1 + ln(10)\n",
    "    M = max(a, b)\n",
    "    val = M + np.log(np.exp(a - M) + np.exp(b - M))\n",
    "    return val\n",
    "\n",
    "def grad_f_stable(x):\n",
    "    x1, x2 = float(x[0]), float(x[1])\n",
    "    a = x1**2 + x2**2\n",
    "    b = x1 + np.log(10.0)\n",
    "    M = max(a, b)\n",
    "    ea = np.exp(a - M)\n",
    "    eb = np.exp(b - M)\n",
    "    denom = ea + eb\n",
    "    # derivadas en escala estable\n",
    "    dfdx1 = (2 * x1 * ea + eb) / denom\n",
    "    dfdx2 = (2 * x2 * ea) / denom\n",
    "    return np.array([dfdx1, dfdx2])\n",
    "\n",
    "def hess_f_stable(x):\n",
    "    # Calculamos el Hessiano de f en forma estable (2x2)\n",
    "    x1, x2 = float(x[0]), float(x[1])\n",
    "    a = x1**2 + x2**2\n",
    "    b = x1 + np.log(10.0)\n",
    "    M = max(a, b)\n",
    "    ea = np.exp(a - M)\n",
    "    eb = np.exp(b - M)\n",
    "    denom = ea + eb\n",
    "\n",
    "    # Componentes auxiliares (numeradores escalados)\n",
    "    A1 = 2 * x1 * ea         # proviene de 2 x1 e^a (escalado)\n",
    "    B1 = eb                  # proviene de 10 e^{x1} (escalado)\n",
    "    num_g1 = A1 + B1         # numerador de g1 (componente x1 del gradiente)\n",
    "    # g2 numerador\n",
    "    num_g2 = 2 * x2 * ea\n",
    "\n",
    "    # derivadas de A1, B1\n",
    "    # dA1/dx1 = 2*ea + 2*x1 * d(ea)/dx1, y d(ea)/dx1 = 2 x1 ea\n",
    "    dA1_dx1 = 2 * ea + 2 * x1 * (2 * x1 * ea)   # 2 ea + 4 x1^2 ea\n",
    "    dA1_dx2 = 2 * x1 * (2 * x2 * ea)             # 4 x1 x2 ea\n",
    "    dB1_dx1 = eb\n",
    "    dB1_dx2 = 0.0\n",
    "\n",
    "    # derivadas del denom\n",
    "    ddenom_dx1 = (2 * x1 * ea) + eb\n",
    "    ddenom_dx2 = (2 * x2 * ea)\n",
    "\n",
    "    # H_11: d(g1)/dx1 por regla del cociente\n",
    "    H11 = ((dA1_dx1 + dB1_dx1) * denom - num_g1 * ddenom_dx1) / (denom**2)\n",
    "\n",
    "    # H_12: d(g1)/dx2\n",
    "    H12 = ((dA1_dx2 + dB1_dx2) * denom - num_g1 * ddenom_dx2) / (denom**2)\n",
    "\n",
    "    # H_21: d(g2)/dx1\n",
    "    # g2 = (2 x2 ea)/denom -> derivada wrt x1:\n",
    "    # numerator derivative: 2 x2 * d(ea)/dx1 = 2 x2 * (2 x1 ea)\n",
    "    dnum_g2_dx1 = 2 * x2 * (2 * x1 * ea)\n",
    "    H21 = (dnum_g2_dx1 * denom - num_g2 * ddenom_dx1) / (denom**2)\n",
    "\n",
    "    # H_22: d(g2)/dx2\n",
    "    # derivative numerator: 2 ea + 2 x2 * d(ea)/dx2 = 2 ea + 4 x2^2 ea\n",
    "    dnum_g2_dx2 = 2 * ea + 4 * x2**2 * ea\n",
    "    H22 = (dnum_g2_dx2 * denom - num_g2 * ddenom_dx2) / (denom**2)\n",
    "\n",
    "    H = np.array([[H11, H12],\n",
    "                  [H21, H22]])\n",
    "    # forzamos simetría numérica\n",
    "    H = 0.5 * (H + H.T)\n",
    "    return H\n",
    "\n",
    "# -----------------------------\n",
    "# Backtracking Armijo line search\n",
    "# -----------------------------\n",
    "def backtracking_armijo(f, grad, xk, pk, alpha0=1.0, c=1e-4, rho=0.5, max_iters=50):\n",
    "    alpha = alpha0\n",
    "    fk = f(xk)\n",
    "    gk = grad(xk)\n",
    "    for _ in range(max_iters):\n",
    "        newx = xk + alpha * pk\n",
    "        if f(newx) <= fk + c * alpha * np.dot(gk, pk):\n",
    "            return alpha\n",
    "        alpha *= rho\n",
    "    return alpha\n",
    "\n",
    "# -----------------------------\n",
    "# Damped Newton / Trust-region style algorithm\n",
    "# (Levenberg-Marquardt style damping + Armijo backtracking)\n",
    "# -----------------------------\n",
    "def damped_newton_trust(f, grad, hess, x0, tol=1e-8, max_iter=200,\n",
    "                        lambda0=1e-3, lambda_factor_increase=10.0, lambda_factor_decrease=0.1):\n",
    "    xk = x0.astype(float).copy()\n",
    "    lamb = lambda0\n",
    "    history = []\n",
    "    for k in range(1, max_iter + 1):\n",
    "        fk = f(xk)\n",
    "        gk = grad(xk)\n",
    "        grad_norm = np.linalg.norm(gk)\n",
    "        history.append((k, xk.copy(), fk, grad_norm, lamb))\n",
    "        if grad_norm < tol:\n",
    "            break\n",
    "\n",
    "        Hk = hess(xk)\n",
    "        success = False\n",
    "\n",
    "        # intentos con damping creciente para asegurar PD y dirección de descenso\n",
    "        for attempt in range(20):\n",
    "            try:\n",
    "                H_reg = Hk + lamb * np.eye(len(xk))\n",
    "                pk = np.linalg.solve(H_reg, -gk)\n",
    "            except np.linalg.LinAlgError:\n",
    "                lamb *= lambda_factor_increase\n",
    "                continue\n",
    "\n",
    "            # si no es dirección de descenso, aumentamos damping\n",
    "            if np.dot(gk, pk) >= 0:\n",
    "                lamb *= lambda_factor_increase\n",
    "                continue\n",
    "\n",
    "            # búsqueda de línea Armijo para el paso propuesto\n",
    "            alpha = backtracking_armijo(f, grad, xk, pk, alpha0=1.0)\n",
    "            newx = xk + alpha * pk\n",
    "            newf = f(newx)\n",
    "\n",
    "            # criterio simple: aceptamos si hay descenso (podemos refinar con ratio tipo trust-region)\n",
    "            if newf <= fk + 1e-12:\n",
    "                success = True\n",
    "                break\n",
    "            else:\n",
    "                lamb *= lambda_factor_increase\n",
    "\n",
    "        if not success:\n",
    "            # fallback: gradiente con paso pequeño si no se encuentra paso Newton estable\n",
    "            pk = -gk\n",
    "            alpha = backtracking_armijo(f, grad, xk, pk, alpha0=1e-3)\n",
    "            xk = xk + alpha * pk\n",
    "            lamb *= lambda_factor_increase\n",
    "            continue\n",
    "\n",
    "        # actualizar x y reducir damping (recuperar comportamiento Newton)\n",
    "        xk = newx\n",
    "        lamb = max(1e-16, lamb * lambda_factor_decrease)\n",
    "\n",
    "    # valores finales\n",
    "    fk = f(xk)\n",
    "    gk = grad(xk)\n",
    "    history.append((\"final\", xk.copy(), fk, np.linalg.norm(gk), lamb))\n",
    "    return xk, fk, np.linalg.norm(gk), k, history\n",
    "\n",
    "# -----------------------------\n",
    "# Ejemplo de ejecución (main)\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1️⃣ Leer los puntos iniciales desde el JSON\n",
    "    with open(\"initial_points.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        # Si el archivo es una lista, la usamos directamente\n",
    "        if isinstance(data, dict):\n",
    "            initials = np.array(data[\"points\"])\n",
    "        else:\n",
    "            initials = np.array(data)\n",
    "\n",
    "    print(f\"Se leyeron {len(initials)} puntos iniciales desde 'initial_points.json'\\n\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # 2️⃣ Ejecutar el método de Newton desde cada punto\n",
    "    for i, x0 in enumerate(initials, 1):\n",
    "        xopt, fopt, gnorm, its, hist = damped_newton_trust(\n",
    "            f_stable, grad_f_stable, hess_f_stable, np.array(x0)\n",
    "        )\n",
    "        results.append({\n",
    "            \"index\": i,\n",
    "            \"x0\": x0.tolist(),\n",
    "            \"x_opt\": xopt.tolist(),\n",
    "            \"f_opt\": float(fopt),\n",
    "            \"grad_norm\": float(gnorm),\n",
    "            \"iterations\": int(its)\n",
    "        })\n",
    "        print(f\"{i:3d}) Init={np.round(x0,3)}  ->  x*={np.round(xopt,6)}  \"\n",
    "              f\"f*={fopt:.8f}  ||grad||={gnorm:.2e}  iters={its}\")\n",
    "\n",
    "    # 3️⃣ Guardar los resultados en un nuevo JSON\n",
    "    output_data = {\"method\": \"damped_newton_trust\", \"results\": results}\n",
    "    with open(\"results_newton.json\", \"w\") as f:\n",
    "        json.dump(output_data, f, indent=4)\n",
    "\n",
    "    print(\"\\n✅ Resultados guardados en 'results_newton.json'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216ffd80",
   "metadata": {},
   "source": [
    "El objetivo es minimizar la función:$$f(x,y) = \\ln(e^{x^2+y^2} + 10 e^{x})$$Cuando los exponentes $x^2+y^2$ o $x$ son muy grandes, $e^{\\text{exponente}}$ puede causar un overflow (desbordamiento) en la computadora. Para evitar esto, se utiliza la técnica Log-Sum-Exp (logaritmo de la suma de exponenciales), que reescribe $\\ln(e^A + e^B)$ como:$$\\ln(e^A + e^B) = \\max(A, B) + \\ln(e^{A-\\max(A, B)} + e^{B-\\max(A, B)})$$Esta forma es numéricamente estable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43050e10",
   "metadata": {},
   "source": [
    "1. Función $f(x)$ (f_stable)\n",
    "- x1, x2 = float(x[0]), float(x[1]): Extrae las componentes $x_1$ y $x_2$ del vector de entrada $x$.\n",
    "\n",
    "- a = $x1^2 + x2^2$: Corresponde al exponente del primer término: $A = x^2+y^2$.\n",
    "\n",
    "- b = x1 + np.log(10.0): Corresponde al exponente del segundo término: $B = x + \\ln(10)$, ya que $10 e^x = e^{\\ln(10)} e^x = e^{x + \\ln(10)}$.\n",
    "\n",
    "- M = max(a, b): Calcula el máximo $M = \\max(A, B)$.\n",
    "\n",
    "- val = M + np.log(np.exp(a - M) + np.exp(b - M)): Implementa la fórmula estable de Log-Sum-Exp.\n",
    "\n",
    "-return val: Devuelve el valor de la función."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a6a25d",
   "metadata": {},
   "source": [
    "2. Gradiente de $f(x)$ (grad_f_stable)\n",
    "\n",
    "Calcula el vector de las primeras derivadas parciales $\\nabla f(x) = (\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2})^T$, utilizando la derivada de la forma estable:$$\\frac{\\partial f}{\\partial x_i} = \\frac{1}{e^{A} + e^{B}} \\cdot (\\frac{\\partial e^{A}}{\\partial x_i} + \\frac{\\partial e^{B}}{\\partial x_i})$$Al escalar por $e^{-M}$ arriba y abajo:$$\\frac{\\partial f}{\\partial x_i} = \\frac{e^{-M}}{e^{-M}} \\frac{e^{-M} (\\frac{\\partial e^{A}}{\\partial x_i} + \\frac{\\partial e^{B}}{\\partial x_i})}{e^{-M} (e^{A} + e^{B})} = \\frac{\\frac{\\partial e^{A}}{\\partial x_i} e^{-M} + \\frac{\\partial e^{B}}{\\partial x_i} e^{-M}}{e^{A-M} + e^{B-M}}$$\n",
    "\n",
    "- ea = np.exp(a - M) y eb = np.exp(b - M): Son los términos escalados $e^{A-M}$ y $e^{B-M}$.\n",
    "\n",
    "- denom = ea + eb: Es el denominador común escalado.\n",
    "- dfdx1 = (2 * x1 * ea + eb) / denom: $\\frac{\\partial f}{\\partial x_1}$. Los numeradores son las derivadas $\\frac{\\partial A}{\\partial x_1} e^{A-M} = 2x_1 e^{A-M}$ y $\\frac{\\partial B}{\\partial x_1} e^{B-M} = 1 \\cdot e^{B-M}$.\n",
    "- dfdx2 = (2 * x2 * ea) / denom: $\\frac{\\partial f}{\\partial x_2}$. Los numeradores son $\\frac{\\partial A}{\\partial x_2} e^{A-M} = 2x_2 e^{A-M}$ y $\\frac{\\partial B}{\\partial x_2} e^{B-M} = 0 \\cdot e^{B-M}$.\n",
    "- return np.array([dfdx1, dfdx2]): Devuelve el gradiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70719131",
   "metadata": {},
   "source": [
    "3. Hessiano de $f(x)$ (hess_f_stable)\n",
    "\n",
    "Calcula la matriz de las segundas derivadas parciales $\\mathbf{H}(x)$, $\\mathbf{H}_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}$. Utiliza la regla del cociente para derivar las componentes del gradiente (que ya están en forma estable).\n",
    "- Calcula términos auxiliares, incluyendo las derivadas de los numeradores y el denominador escalados, para luego aplicar la regla del cociente: $\\frac{d}{dx} \\left(\\frac{u}{v}\\right) = \\frac{u'v - uv'}{v^2}$.\n",
    "- H11, H12, H21, H22: Son las cuatro componentes del Hessiano.\n",
    "- H = np.array([[H11, H12], [H21, H22]]): Ensambla la matriz Hessiana.\n",
    "- H = 0.5 * (H + H.T): Fuerza la simetría numérica del Hessiano, lo cual es teórico para funciones suaves (Teorema de Clairaut).\n",
    "- return H: Devuelve la matriz Hessiana."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e247ed",
   "metadata": {},
   "source": [
    "4. Búsqueda de Línea Armijo (backtracking_armijo)\n",
    "Esta función se utiliza para encontrar un tamaño de paso ($\\alpha$) que asegure un descenso suficiente en el valor de la función a lo largo de la dirección de búsqueda ($p_k$).\n",
    "- Entradas: La función $f$, el gradiente grad, el punto actual $x_k$, y la dirección de búsqueda $p_k$.\n",
    "- Criterio de Armijo: Busca un $\\alpha$ (empezando por alpha0=1.0) tal que:$$f(x_k + \\alpha p_k) \\le f(x_k) + c \\cdot \\alpha \\cdot \\nabla f(x_k)^T p_k$$Donde $c$ (c=1e-4) es un factor de descenso pequeño.\n",
    "- Mecanismo: Si el criterio no se cumple, el paso $\\alpha$ se reduce multiplicándolo por $\\rho$ (rho=0.5) (proceso de backtracking o retroceso), y se repite hasta que el criterio se cumpla o se alcance max_iters.\n",
    "- return alpha: Devuelve el paso $\\alpha$ encontrado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2824a167",
   "metadata": {},
   "source": [
    "5. Newton Amortiguado (damped_newton_trust)\n",
    "\n",
    "Implementa el método de Newton amortiguado, que es una mezcla entre el método de Newton y el método de Máximo Descenso. Se utiliza un término de amortiguamiento (damping) $\\lambda$ (similar al algoritmo de Levenberg-Marquardt o un enfoque tipo Trust-Region) para asegurar que la dirección de búsqueda sea una dirección de descenso.\n",
    "- Bucle Principal: Se repite hasta que la norma del gradiente (grad_norm) sea menor que la tolerancia tol o se alcance max_iter.\n",
    "- Cálculo de la Dirección de Newton Amortiguada:\n",
    "   - H_reg = Hk + lamb * np.eye(len(xk)): Se añade el término de amortiguamiento $\\lambda$ a la diagonal del Hessiano $H_k$, creando una matriz regularizada $H_{reg}$. Esto asegura que $H_{reg}$ sea Definida Positiva (o al menos mejor condicionada) y la dirección $p_k$ calculada sea una dirección de descenso.\n",
    "   - pk = np.linalg.solve(H_reg, -gk): Resuelve el sistema lineal para encontrar la dirección de Newton amortiguada $p_k$.\n",
    "- Manejo de Fallos (Damping Adjustment):\n",
    "   - Si la solución falla (LinAlgError), o si la dirección no es de descenso (np.dot(gk, pk) >= 0), $\\lambda$ se aumenta (lambda *= lambda_factor_increase), y se intenta de nuevo (aumentando el damping).\n",
    "- Búsqueda de Línea y Actualización:\n",
    "   - Si se encuentra una dirección de descenso, se usa backtracking_armijo para encontrar el tamaño de paso $\\alpha$.\n",
    "   - newx = xk + alpha * pk: Se calcula el nuevo punto.\n",
    "   - Si el nuevo punto reduce el valor de la función (newf <= fk + 1e-12), el paso es exitoso (success = True):\n",
    "       - xk = newx: Se acepta el nuevo punto.\n",
    "       - lamb se reduce (lamb = max(1e-16, lamb * lambda_factor_decrease)) para recuperar el comportamiento de Newton puro (convergencia cuadrática).\n",
    "   - Si no hay descenso, $\\lambda$ se aumenta nuevamente.\n",
    "- Fallback (Retroceso): Si el bucle de intentos (attempt in range(20)) falla en encontrar un paso Newton aceptable, se utiliza un paso de máximo descenso (pk = -gk) con un $\\alpha$ muy pequeño.\n",
    "- return xk, fk, np.linalg.norm(gk), k, history: Devuelve el punto óptimo, el valor de la función, la norma final del gradiente, el número de iteraciones y un historial de convergencia."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
