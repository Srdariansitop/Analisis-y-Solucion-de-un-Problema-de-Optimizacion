{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8927fe51",
   "metadata": {},
   "source": [
    "Aplicacion del Algoritmo BFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f12e87e",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Definici√≥n de la funci√≥n, gradiente y log-sum-exp seguro\n",
    "# ---------------------------------------------------------\n",
    "def f(x):\n",
    "    # x es un vector [x0, x1]\n",
    "    x1, x2 = x\n",
    "    A = np.exp(x1**2 + x2**2) + 10 * np.exp(x1)\n",
    "    return np.log(A)\n",
    "\n",
    "def grad_f(x):\n",
    "    x1, x2 = x\n",
    "    A = np.exp(x1**2 + x2**2) + 10 * np.exp(x1)\n",
    "    dfdx1 = (2 * x1 * np.exp(x1**2 + x2**2) + 10 * np.exp(x1)) / A\n",
    "    dfdx2 = (2 * x2 * np.exp(x1**2 + x2**2)) / A\n",
    "    return np.array([dfdx1, dfdx2])\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# B√∫squeda de l√≠nea con condici√≥n de Armijo\n",
    "# ---------------------------------------------------------\n",
    "def line_search_armijo(f, grad_f, xk, pk, alpha0=1.0, c=1e-4, rho=0.5):\n",
    "    alpha = alpha0\n",
    "    fk = f(xk)\n",
    "    gradk = grad_f(xk)\n",
    "    while f(xk + alpha * pk) > fk + c * alpha * np.dot(gradk, pk):\n",
    "        alpha *= rho\n",
    "        if alpha < 1e-10:\n",
    "            break\n",
    "    return alpha\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# M√©todo BFGS\n",
    "# ---------------------------------------------------------\n",
    "def bfgs(f, grad_f, x0, tol=1e-6, max_iter=1000):\n",
    "    xk = x0.copy()\n",
    "    n = len(xk)\n",
    "    Hk = np.eye(n)  # matriz de aproximaci√≥n del inverso del Hessiano\n",
    "    fk = f(xk)\n",
    "    gk = grad_f(xk)\n",
    "    iter_data = [(0, xk.copy(), fk, np.linalg.norm(gk))]\n",
    "\n",
    "    for k in range(1, max_iter + 1):\n",
    "        # Direcci√≥n de b√∫squeda\n",
    "        pk = -Hk.dot(gk)\n",
    "\n",
    "        # B√∫squeda de l√≠nea (Armijo)\n",
    "        alpha = line_search_armijo(f, grad_f, xk, pk)\n",
    "\n",
    "        # Actualizaci√≥n de x\n",
    "        x_new = xk + alpha * pk\n",
    "        g_new = grad_f(x_new)\n",
    "        s = x_new - xk\n",
    "        y = g_new - gk\n",
    "\n",
    "        # Condici√≥n de actualizaci√≥n BFGS (evitar divisiones malas)\n",
    "        if np.dot(y, s) > 1e-10:\n",
    "            rho = 1.0 / np.dot(y, s)\n",
    "            I = np.eye(n)\n",
    "            Hk = (I - rho * np.outer(s, y)) @ Hk @ (I - rho * np.outer(y, s)) + rho * np.outer(s, s)\n",
    "\n",
    "        # Actualizar variables\n",
    "        xk, gk, fk = x_new, g_new, f(x_new)\n",
    "        iter_data.append((k, xk.copy(), fk, np.linalg.norm(gk)))\n",
    "\n",
    "        # Criterios de paro\n",
    "        if np.linalg.norm(gk) < tol:\n",
    "            break\n",
    "\n",
    "    return xk, fk, np.linalg.norm(gk), k, iter_data\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Ejemplo de ejecuci√≥n\n",
    "# ---------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    x0 = np.array([0.0, 0.0])   # punto inicial\n",
    "    x_opt, f_opt, grad_norm, iters, history = bfgs(f, grad_f, x0)\n",
    "\n",
    "    print(\"Resultado BFGS:\")\n",
    "    print(f\"  x* = {x_opt}\")\n",
    "    print(f\"  f(x*) = {f_opt:.6f}\")\n",
    "    print(f\"  ||grad|| = {grad_norm:.2e}\")\n",
    "    print(f\"  Iteraciones = {iters}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5fbdb8",
   "metadata": {},
   "source": [
    "El script est√° organizado en tres bloques funcionales:\n",
    "\n",
    "Definici√≥n del problema: f(x) y grad_f(x).\n",
    "\n",
    "B√∫squeda de l√≠nea: line_search_armijo(...).\n",
    "\n",
    "Algoritmo BFGS: bfgs(...) que llama a la b√∫squeda de l√≠nea y actualiza la aproximaci√≥n del inverso del Hessiano.\n",
    "Al final hay un if __name__ == \"__main__\": con un ejemplo de ejecuci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c8ab31",
   "metadata": {},
   "source": [
    "1. Explicacion de la funcion :\n",
    "\n",
    "Qu√© hace: grad_f(x) Calcula ‚àáf usando las f√≥rmulas anal√≠ticas que derivamos.\n",
    "\n",
    "Por qu√© usar el gradiente anal√≠tico: BFGS es un m√©todo cuasi-Newton que requiere gradientes para construir las actualizaciones. Calcular el gradiente anal√≠ticamente es m√°s preciso y r√°pido que aproximarlo por diferencias finitas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5691d9",
   "metadata": {},
   "source": [
    "2. Explicacion de la funcion :\n",
    "\n",
    "line_search_armijo(...) ‚Äî b√∫squeda de l√≠nea simple (Armijo / backtracking)\n",
    "\n",
    "Qu√© hace: \n",
    "- Intenta alpha=1 y si el nuevo punto no satisface la condici√≥n de Armijo (descenso suficiente), reduce alpha multiplic√°ndolo por rho (0.5) repetidamente (backtracking).\n",
    "\n",
    "Par√°metros importantes:\n",
    "\n",
    "c: constante de Armijo (t√≠pico 10^(-4))\n",
    "\n",
    "rho: factor de reducci√≥n (0.5) ‚Äî cada iteraci√≥n divide alpha por 2.\n",
    "\n",
    "Por qu√© usar Armijo simple: es f√°cil de implementar y suele ser suficiente para BFGS en problemas suaves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429d9f79",
   "metadata": {},
   "source": [
    "3. Explicacion de la funcion :\n",
    "\n",
    "bfgs(f, grad_f, x0, tol=1e-6, max_iter=1000): - El nucleo del algoritmo\n",
    "\n",
    "Inicializaci√≥n\n",
    "\n",
    "Hk = I: asumimos inicialmente que el inverso del Hessiano es identidad (soluci√≥n neutra).\n",
    "\n",
    "gk = grad_f(xk): empezamos con el gradiente en el punto inicial.\n",
    "\n",
    "iter_data: se guarda historia para an√°lisis o trazados.\n",
    "\n",
    "Direcci√≥n de b√∫squeda pk\n",
    "\n",
    "pk = -Hk.dot(gk) produce una direcci√≥n que incorpora informaci√≥n aproximada de curvatura; si Hk=I, pk es la direcci√≥n de descenso por gradiente.\n",
    "\n",
    "B√∫squeda de l√≠nea\n",
    "\n",
    "Se usa la funci√≥n Armijo descrita. El tama√±o alpha determina cu√°nto avanzar.\n",
    "\n",
    "Actualizar s y y\n",
    "\n",
    "s = x_{k+1} - x_k y y = g_{k+1} - g_k son los vectores que BFGS usa para actualizar la matriz inversa del Hessiano.\n",
    "\n",
    "Condici√≥n y^T s > 0\n",
    "\n",
    "El requisito te√≥rico para BFGS es ùë¶^(‚ä§) * s>0 (curvatura positiva). Si no se cumple, la f√≥rmula est√°ndar puede introducir indefinidades o dividir por cero.\n",
    "\n",
    "En el c√≥digo se exige > 1e-10 (peque√±a tolerancia) para evitar divisiones num√©ricas malas; si no se cumple, se omite la actualizaci√≥n (o en implementaciones avanzadas se modifica y o se reinicia Hk).\n",
    "\n",
    "Esta salvaguarda es pr√°ctica y frecuente.\n",
    "\n",
    "F√≥rmula de actualizaci√≥n\n",
    "\n",
    "La actualizaci√≥n implementada es la forma cl√°sica del BFGS para la aproximaci√≥n del inverso del Hessiano:\n",
    "\n",
    "$$\n",
    "    \\mathbf{H}_{k+1} = \\left(\\mathbf{I} - \\rho_k \\mathbf{s}_k \\mathbf{y}_k^T\\right)\\mathbf{H}_k\\left(\\mathbf{I} - \\rho_k \\mathbf{y}_k \\mathbf{s}_k^T\\right) + \\rho_k \\mathbf{s}_k \\mathbf{s}_k^T\n",
    "$$\n",
    "Donde el escalar $\\rho_k$ se define como:\n",
    "$$\n",
    "    \\rho_k = \\frac{1}{\\mathbf{y}_k^T \\mathbf{s}_k}\n",
    "$$\n",
    "\n",
    "Es la forma que mantiene simetr√≠a y (si se cumple la condici√≥n de curvatura) positividad definida de ùêªùëò+1\n",
    "\n",
    "Criterios de paro\n",
    "\n",
    "np.linalg.norm(gk) < tol: norma del gradiente por debajo del umbral ‚Üí detenido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000e9851",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de366071",
   "metadata": {},
   "source": [
    "Algoritmo de Newton amortiguado (Damped Newton / Trust-Region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b942ed10",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9825f19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluaci√≥n num√©ricamente estable de f, gradiente y Hessiano\n",
    "# f(x,y) = ln(e^{x^2+y^2} + 10 e^{x})\n",
    "# Usamos log-sum-exp para evitar overflow.\n",
    "# -----------------------------\n",
    "def f_stable(x):\n",
    "    x1, x2 = float(x[0]), float(x[1])\n",
    "    a = x1**2 + x2**2\n",
    "    b = x1 + np.log(10.0)    # log(10 * e^{x1}) = x1 + ln(10)\n",
    "    M = max(a, b)\n",
    "    val = M + np.log(np.exp(a - M) + np.exp(b - M))\n",
    "    return val\n",
    "\n",
    "def grad_f_stable(x):\n",
    "    x1, x2 = float(x[0]), float(x[1])\n",
    "    a = x1**2 + x2**2\n",
    "    b = x1 + np.log(10.0)\n",
    "    M = max(a, b)\n",
    "    ea = np.exp(a - M)\n",
    "    eb = np.exp(b - M)\n",
    "    denom = ea + eb\n",
    "    # derivadas en escala estable\n",
    "    dfdx1 = (2 * x1 * ea + eb) / denom\n",
    "    dfdx2 = (2 * x2 * ea) / denom\n",
    "    return np.array([dfdx1, dfdx2])\n",
    "\n",
    "def hess_f_stable(x):\n",
    "    # Calculamos el Hessiano de f en forma estable (2x2)\n",
    "    x1, x2 = float(x[0]), float(x[1])\n",
    "    a = x1**2 + x2**2\n",
    "    b = x1 + np.log(10.0)\n",
    "    M = max(a, b)\n",
    "    ea = np.exp(a - M)\n",
    "    eb = np.exp(b - M)\n",
    "    denom = ea + eb\n",
    "\n",
    "    # Componentes auxiliares (numeradores escalados)\n",
    "    A1 = 2 * x1 * ea         # proviene de 2 x1 e^a (escalado)\n",
    "    B1 = eb                  # proviene de 10 e^{x1} (escalado)\n",
    "    num_g1 = A1 + B1         # numerador de g1 (componente x1 del gradiente)\n",
    "    # g2 numerador\n",
    "    num_g2 = 2 * x2 * ea\n",
    "\n",
    "    # derivadas de A1, B1\n",
    "    # dA1/dx1 = 2*ea + 2*x1 * d(ea)/dx1, y d(ea)/dx1 = 2 x1 ea\n",
    "    dA1_dx1 = 2 * ea + 2 * x1 * (2 * x1 * ea)   # 2 ea + 4 x1^2 ea\n",
    "    dA1_dx2 = 2 * x1 * (2 * x2 * ea)             # 4 x1 x2 ea\n",
    "    dB1_dx1 = eb\n",
    "    dB1_dx2 = 0.0\n",
    "\n",
    "    # derivadas del denom\n",
    "    ddenom_dx1 = (2 * x1 * ea) + eb\n",
    "    ddenom_dx2 = (2 * x2 * ea)\n",
    "\n",
    "    # H_11: d(g1)/dx1 por regla del cociente\n",
    "    H11 = ((dA1_dx1 + dB1_dx1) * denom - num_g1 * ddenom_dx1) / (denom**2)\n",
    "\n",
    "    # H_12: d(g1)/dx2\n",
    "    H12 = ((dA1_dx2 + dB1_dx2) * denom - num_g1 * ddenom_dx2) / (denom**2)\n",
    "\n",
    "    # H_21: d(g2)/dx1\n",
    "    # g2 = (2 x2 ea)/denom -> derivada wrt x1:\n",
    "    # numerator derivative: 2 x2 * d(ea)/dx1 = 2 x2 * (2 x1 ea)\n",
    "    dnum_g2_dx1 = 2 * x2 * (2 * x1 * ea)\n",
    "    H21 = (dnum_g2_dx1 * denom - num_g2 * ddenom_dx1) / (denom**2)\n",
    "\n",
    "    # H_22: d(g2)/dx2\n",
    "    # derivative numerator: 2 ea + 2 x2 * d(ea)/dx2 = 2 ea + 4 x2^2 ea\n",
    "    dnum_g2_dx2 = 2 * ea + 4 * x2**2 * ea\n",
    "    H22 = (dnum_g2_dx2 * denom - num_g2 * ddenom_dx2) / (denom**2)\n",
    "\n",
    "    H = np.array([[H11, H12],\n",
    "                  [H21, H22]])\n",
    "    # forzamos simetr√≠a num√©rica\n",
    "    H = 0.5 * (H + H.T)\n",
    "    return H\n",
    "\n",
    "# -----------------------------\n",
    "# Backtracking Armijo line search\n",
    "# -----------------------------\n",
    "def backtracking_armijo(f, grad, xk, pk, alpha0=1.0, c=1e-4, rho=0.5, max_iters=50):\n",
    "    alpha = alpha0\n",
    "    fk = f(xk)\n",
    "    gk = grad(xk)\n",
    "    for _ in range(max_iters):\n",
    "        newx = xk + alpha * pk\n",
    "        if f(newx) <= fk + c * alpha * np.dot(gk, pk):\n",
    "            return alpha\n",
    "        alpha *= rho\n",
    "    return alpha\n",
    "\n",
    "# -----------------------------\n",
    "# Damped Newton / Trust-region style algorithm\n",
    "# (Levenberg-Marquardt style damping + Armijo backtracking)\n",
    "# -----------------------------\n",
    "def damped_newton_trust(f, grad, hess, x0, tol=1e-8, max_iter=200,\n",
    "                        lambda0=1e-3, lambda_factor_increase=10.0, lambda_factor_decrease=0.1):\n",
    "    xk = x0.astype(float).copy()\n",
    "    lamb = lambda0\n",
    "    history = []\n",
    "    for k in range(1, max_iter + 1):\n",
    "        fk = f(xk)\n",
    "        gk = grad(xk)\n",
    "        grad_norm = np.linalg.norm(gk)\n",
    "        history.append((k, xk.copy(), fk, grad_norm, lamb))\n",
    "        if grad_norm < tol:\n",
    "            break\n",
    "\n",
    "        Hk = hess(xk)\n",
    "        success = False\n",
    "\n",
    "        # intentos con damping creciente para asegurar PD y direcci√≥n de descenso\n",
    "        for attempt in range(20):\n",
    "            try:\n",
    "                H_reg = Hk + lamb * np.eye(len(xk))\n",
    "                pk = np.linalg.solve(H_reg, -gk)\n",
    "            except np.linalg.LinAlgError:\n",
    "                lamb *= lambda_factor_increase\n",
    "                continue\n",
    "\n",
    "            # si no es direcci√≥n de descenso, aumentamos damping\n",
    "            if np.dot(gk, pk) >= 0:\n",
    "                lamb *= lambda_factor_increase\n",
    "                continue\n",
    "\n",
    "            # b√∫squeda de l√≠nea Armijo para el paso propuesto\n",
    "            alpha = backtracking_armijo(f, grad, xk, pk, alpha0=1.0)\n",
    "            newx = xk + alpha * pk\n",
    "            newf = f(newx)\n",
    "\n",
    "            # criterio simple: aceptamos si hay descenso (podemos refinar con ratio tipo trust-region)\n",
    "            if newf <= fk + 1e-12:\n",
    "                success = True\n",
    "                break\n",
    "            else:\n",
    "                lamb *= lambda_factor_increase\n",
    "\n",
    "        if not success:\n",
    "            # fallback: gradiente con paso peque√±o si no se encuentra paso Newton estable\n",
    "            pk = -gk\n",
    "            alpha = backtracking_armijo(f, grad, xk, pk, alpha0=1e-3)\n",
    "            xk = xk + alpha * pk\n",
    "            lamb *= lambda_factor_increase\n",
    "            continue\n",
    "\n",
    "        # actualizar x y reducir damping (recuperar comportamiento Newton)\n",
    "        xk = newx\n",
    "        lamb = max(1e-16, lamb * lambda_factor_decrease)\n",
    "\n",
    "    # valores finales\n",
    "    fk = f(xk)\n",
    "    gk = grad(xk)\n",
    "    history.append((\"final\", xk.copy(), fk, np.linalg.norm(gk), lamb))\n",
    "    return xk, fk, np.linalg.norm(gk), k, history\n",
    "\n",
    "# -----------------------------\n",
    "# Ejemplo de ejecuci√≥n (main)\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    initials = [\n",
    "        np.array([0.0, 0.0]),\n",
    "        np.array([1.0, 0.0]),\n",
    "        np.array([-1.0, 2.0]),\n",
    "        np.array([2.0, 2.0])\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for x0 in initials:\n",
    "        xopt, fopt, gnorm, its, hist = damped_newton_trust(\n",
    "            f_stable, grad_f_stable, hess_f_stable, x0\n",
    "        )\n",
    "        print(\"Init:\", x0, \"-> x*:\", np.round(xopt, 6),\n",
    "              \" f*:\", np.round(fopt, 8), \"||grad||:\", np.format_float_scientific(gnorm, 2),\n",
    "              \"iter:\", its)\n",
    "        results.append((x0, xopt, fopt, gnorm, its))\n",
    "\n",
    "    # Graficar convergencia (norma del gradiente) del √∫ltimo historial como ejemplo\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        hist = hist  # historial de la √∫ltima ejecuci√≥n\n",
    "        iter_nums = [h[0] for h in hist if isinstance(h[0], int)]\n",
    "        grad_norms = [h[3] for h in hist if isinstance(h[0], int)]\n",
    "        if len(iter_nums) > 0:\n",
    "            plt.semilogy(iter_nums, grad_norms, marker='o')\n",
    "            plt.xlabel('Iteraci√≥n')\n",
    "            plt.ylabel('||grad|| (escala log)')\n",
    "            plt.title('Convergencia (ejemplo)')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "    except Exception:\n",
    "        # si no hay matplotlib, no graficamos pero el algoritmo sigue funcionando\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216ffd80",
   "metadata": {},
   "source": [
    "El objetivo es minimizar la funci√≥n:$$f(x,y) = \\ln(e^{x^2+y^2} + 10 e^{x})$$Cuando los exponentes $x^2+y^2$ o $x$ son muy grandes, $e^{\\text{exponente}}$ puede causar un overflow (desbordamiento) en la computadora. Para evitar esto, se utiliza la t√©cnica Log-Sum-Exp (logaritmo de la suma de exponenciales), que reescribe $\\ln(e^A + e^B)$ como:$$\\ln(e^A + e^B) = \\max(A, B) + \\ln(e^{A-\\max(A, B)} + e^{B-\\max(A, B)})$$Esta forma es num√©ricamente estable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43050e10",
   "metadata": {},
   "source": [
    "1. Funci√≥n $f(x)$ (f_stable)\n",
    "- x1, x2 = float(x[0]), float(x[1]): Extrae las componentes $x_1$ y $x_2$ del vector de entrada $x$.\n",
    "\n",
    "- a = $x1^2 + x2^2$: Corresponde al exponente del primer t√©rmino: $A = x^2+y^2$.\n",
    "\n",
    "- b = x1 + np.log(10.0): Corresponde al exponente del segundo t√©rmino: $B = x + \\ln(10)$, ya que $10 e^x = e^{\\ln(10)} e^x = e^{x + \\ln(10)}$.\n",
    "\n",
    "- M = max(a, b): Calcula el m√°ximo $M = \\max(A, B)$.\n",
    "\n",
    "- val = M + np.log(np.exp(a - M) + np.exp(b - M)): Implementa la f√≥rmula estable de Log-Sum-Exp.\n",
    "\n",
    "-return val: Devuelve el valor de la funci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a6a25d",
   "metadata": {},
   "source": [
    "2. Gradiente de $f(x)$ (grad_f_stable)\n",
    "\n",
    "Calcula el vector de las primeras derivadas parciales $\\nabla f(x) = (\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2})^T$, utilizando la derivada de la forma estable:$$\\frac{\\partial f}{\\partial x_i} = \\frac{1}{e^{A} + e^{B}} \\cdot (\\frac{\\partial e^{A}}{\\partial x_i} + \\frac{\\partial e^{B}}{\\partial x_i})$$Al escalar por $e^{-M}$ arriba y abajo:$$\\frac{\\partial f}{\\partial x_i} = \\frac{e^{-M}}{e^{-M}} \\frac{e^{-M} (\\frac{\\partial e^{A}}{\\partial x_i} + \\frac{\\partial e^{B}}{\\partial x_i})}{e^{-M} (e^{A} + e^{B})} = \\frac{\\frac{\\partial e^{A}}{\\partial x_i} e^{-M} + \\frac{\\partial e^{B}}{\\partial x_i} e^{-M}}{e^{A-M} + e^{B-M}}$$\n",
    "\n",
    "- ea = np.exp(a - M) y eb = np.exp(b - M): Son los t√©rminos escalados $e^{A-M}$ y $e^{B-M}$.\n",
    "\n",
    "- denom = ea + eb: Es el denominador com√∫n escalado.\n",
    "- dfdx1 = (2 * x1 * ea + eb) / denom: $\\frac{\\partial f}{\\partial x_1}$. Los numeradores son las derivadas $\\frac{\\partial A}{\\partial x_1} e^{A-M} = 2x_1 e^{A-M}$ y $\\frac{\\partial B}{\\partial x_1} e^{B-M} = 1 \\cdot e^{B-M}$.\n",
    "- dfdx2 = (2 * x2 * ea) / denom: $\\frac{\\partial f}{\\partial x_2}$. Los numeradores son $\\frac{\\partial A}{\\partial x_2} e^{A-M} = 2x_2 e^{A-M}$ y $\\frac{\\partial B}{\\partial x_2} e^{B-M} = 0 \\cdot e^{B-M}$.\n",
    "- return np.array([dfdx1, dfdx2]): Devuelve el gradiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70719131",
   "metadata": {},
   "source": [
    "3. Hessiano de $f(x)$ (hess_f_stable)\n",
    "\n",
    "Calcula la matriz de las segundas derivadas parciales $\\mathbf{H}(x)$, $\\mathbf{H}_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}$. Utiliza la regla del cociente para derivar las componentes del gradiente (que ya est√°n en forma estable).\n",
    "- Calcula t√©rminos auxiliares, incluyendo las derivadas de los numeradores y el denominador escalados, para luego aplicar la regla del cociente: $\\frac{d}{dx} \\left(\\frac{u}{v}\\right) = \\frac{u'v - uv'}{v^2}$.\n",
    "- H11, H12, H21, H22: Son las cuatro componentes del Hessiano.\n",
    "- H = np.array([[H11, H12], [H21, H22]]): Ensambla la matriz Hessiana.\n",
    "- H = 0.5 * (H + H.T): Fuerza la simetr√≠a num√©rica del Hessiano, lo cual es te√≥rico para funciones suaves (Teorema de Clairaut).\n",
    "- return H: Devuelve la matriz Hessiana."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e247ed",
   "metadata": {},
   "source": [
    "4. B√∫squeda de L√≠nea Armijo (backtracking_armijo)\n",
    "Esta funci√≥n se utiliza para encontrar un tama√±o de paso ($\\alpha$) que asegure un descenso suficiente en el valor de la funci√≥n a lo largo de la direcci√≥n de b√∫squeda ($p_k$).\n",
    "- Entradas: La funci√≥n $f$, el gradiente grad, el punto actual $x_k$, y la direcci√≥n de b√∫squeda $p_k$.\n",
    "- Criterio de Armijo: Busca un $\\alpha$ (empezando por alpha0=1.0) tal que:$$f(x_k + \\alpha p_k) \\le f(x_k) + c \\cdot \\alpha \\cdot \\nabla f(x_k)^T p_k$$Donde $c$ (c=1e-4) es un factor de descenso peque√±o.\n",
    "- Mecanismo: Si el criterio no se cumple, el paso $\\alpha$ se reduce multiplic√°ndolo por $\\rho$ (rho=0.5) (proceso de backtracking o retroceso), y se repite hasta que el criterio se cumpla o se alcance max_iters.\n",
    "- return alpha: Devuelve el paso $\\alpha$ encontrado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2824a167",
   "metadata": {},
   "source": [
    "5. Newton Amortiguado (damped_newton_trust)\n",
    "\n",
    "Implementa el m√©todo de Newton amortiguado, que es una mezcla entre el m√©todo de Newton y el m√©todo de M√°ximo Descenso. Se utiliza un t√©rmino de amortiguamiento (damping) $\\lambda$ (similar al algoritmo de Levenberg-Marquardt o un enfoque tipo Trust-Region) para asegurar que la direcci√≥n de b√∫squeda sea una direcci√≥n de descenso.\n",
    "- Bucle Principal: Se repite hasta que la norma del gradiente (grad_norm) sea menor que la tolerancia tol o se alcance max_iter.\n",
    "- C√°lculo de la Direcci√≥n de Newton Amortiguada:\n",
    "   - H_reg = Hk + lamb * np.eye(len(xk)): Se a√±ade el t√©rmino de amortiguamiento $\\lambda$ a la diagonal del Hessiano $H_k$, creando una matriz regularizada $H_{reg}$. Esto asegura que $H_{reg}$ sea Definida Positiva (o al menos mejor condicionada) y la direcci√≥n $p_k$ calculada sea una direcci√≥n de descenso.\n",
    "   - pk = np.linalg.solve(H_reg, -gk): Resuelve el sistema lineal para encontrar la direcci√≥n de Newton amortiguada $p_k$.\n",
    "- Manejo de Fallos (Damping Adjustment):\n",
    "   - Si la soluci√≥n falla (LinAlgError), o si la direcci√≥n no es de descenso (np.dot(gk, pk) >= 0), $\\lambda$ se aumenta (lambda *= lambda_factor_increase), y se intenta de nuevo (aumentando el damping).\n",
    "- B√∫squeda de L√≠nea y Actualizaci√≥n:\n",
    "   - Si se encuentra una direcci√≥n de descenso, se usa backtracking_armijo para encontrar el tama√±o de paso $\\alpha$.\n",
    "   - newx = xk + alpha * pk: Se calcula el nuevo punto.\n",
    "   - Si el nuevo punto reduce el valor de la funci√≥n (newf <= fk + 1e-12), el paso es exitoso (success = True):\n",
    "       - xk = newx: Se acepta el nuevo punto.\n",
    "       - lamb se reduce (lamb = max(1e-16, lamb * lambda_factor_decrease)) para recuperar el comportamiento de Newton puro (convergencia cuadr√°tica).\n",
    "   - Si no hay descenso, $\\lambda$ se aumenta nuevamente.\n",
    "- Fallback (Retroceso): Si el bucle de intentos (attempt in range(20)) falla en encontrar un paso Newton aceptable, se utiliza un paso de m√°ximo descenso (pk = -gk) con un $\\alpha$ muy peque√±o.\n",
    "- return xk, fk, np.linalg.norm(gk), k, history: Devuelve el punto √≥ptimo, el valor de la funci√≥n, la norma final del gradiente, el n√∫mero de iteraciones y un historial de convergencia."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
